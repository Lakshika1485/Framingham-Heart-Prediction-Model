{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6963f1c-f2e4-4d26-92d5-4c968c7189c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbmNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading lightgbm-4.6.0-py3-none-win_amd64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in d:\\anaconda\\lib\\site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in d:\\anaconda\\lib\\site-packages (from lightgbm) (1.13.1)\n",
      "Downloading lightgbm-4.6.0-py3-none-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.1/1.5 MB 3.0 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.6/1.5 MB 7.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 9.2 MB/s eta 0:00:00\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n"
     ]
    }
   ],
   "source": [
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "144f849f-b18a-42a8-b5a0-9aa2ac8e02f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\sklearn\\experimental\\enable_hist_gradient_boosting.py:15: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split,KFold\n",
    "from sklearn.ensemble import RandomForestClassifier,VotingClassifier,StackingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, StackingClassifier\n",
    "from lightgbm import LGBMClassifier,early_stopping\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score,log_loss\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e13098f-f389-4dc2-83ec-a3a6fcf8270d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: clustered-imputation==1.0.2 in d:\\anaconda\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from clustered-imputation==1.0.2) (1.26.4)\n",
      "Requirement already satisfied: pandas in d:\\anaconda\\lib\\site-packages (from clustered-imputation==1.0.2) (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in d:\\anaconda\\lib\\site-packages (from clustered-imputation==1.0.2) (1.4.2)\n",
      "Requirement already satisfied: fancyimpute in d:\\anaconda\\lib\\site-packages (from clustered-imputation==1.0.2) (0.7.0)\n",
      "Requirement already satisfied: knnimpute>=0.1.0 in d:\\anaconda\\lib\\site-packages (from fancyimpute->clustered-imputation==1.0.2) (0.1.0)\n",
      "Requirement already satisfied: cvxpy in d:\\anaconda\\lib\\site-packages (from fancyimpute->clustered-imputation==1.0.2) (1.6.5)\n",
      "Requirement already satisfied: cvxopt in d:\\anaconda\\lib\\site-packages (from fancyimpute->clustered-imputation==1.0.2) (1.3.2)\n",
      "Requirement already satisfied: pytest in d:\\anaconda\\lib\\site-packages (from fancyimpute->clustered-imputation==1.0.2) (7.4.4)\n",
      "Requirement already satisfied: nose in d:\\anaconda\\lib\\site-packages (from fancyimpute->clustered-imputation==1.0.2) (1.3.7)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\anaconda\\lib\\site-packages (from scikit-learn->clustered-imputation==1.0.2) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\anaconda\\lib\\site-packages (from scikit-learn->clustered-imputation==1.0.2) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\anaconda\\lib\\site-packages (from scikit-learn->clustered-imputation==1.0.2) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\anaconda\\lib\\site-packages (from pandas->clustered-imputation==1.0.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda\\lib\\site-packages (from pandas->clustered-imputation==1.0.2) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\anaconda\\lib\\site-packages (from pandas->clustered-imputation==1.0.2) (2023.3)\n",
      "Requirement already satisfied: six in d:\\anaconda\\lib\\site-packages (from knnimpute>=0.1.0->fancyimpute->clustered-imputation==1.0.2) (1.16.0)\n",
      "Requirement already satisfied: osqp>=0.6.2 in d:\\anaconda\\lib\\site-packages (from cvxpy->fancyimpute->clustered-imputation==1.0.2) (1.0.4)\n",
      "Requirement already satisfied: clarabel>=0.5.0 in d:\\anaconda\\lib\\site-packages (from cvxpy->fancyimpute->clustered-imputation==1.0.2) (0.10.0)\n",
      "Requirement already satisfied: scs>=3.2.4.post1 in d:\\anaconda\\lib\\site-packages (from cvxpy->fancyimpute->clustered-imputation==1.0.2) (3.2.7.post2)\n",
      "Requirement already satisfied: iniconfig in d:\\anaconda\\lib\\site-packages (from pytest->fancyimpute->clustered-imputation==1.0.2) (1.1.1)\n",
      "Requirement already satisfied: packaging in d:\\anaconda\\lib\\site-packages (from pytest->fancyimpute->clustered-imputation==1.0.2) (23.2)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in d:\\anaconda\\lib\\site-packages (from pytest->fancyimpute->clustered-imputation==1.0.2) (1.0.0)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from pytest->fancyimpute->clustered-imputation==1.0.2) (0.4.6)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\lib\\site-packages (from osqp>=0.6.2->cvxpy->fancyimpute->clustered-imputation==1.0.2) (3.1.4)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from osqp>=0.6.2->cvxpy->fancyimpute->clustered-imputation==1.0.2) (69.5.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda\\lib\\site-packages (from jinja2->osqp>=0.6.2->cvxpy->fancyimpute->clustered-imputation==1.0.2) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install clustered-imputation==1.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97c28ad6-0de4-4b45-9d39-ade3a09930d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"framingham.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70d70e40-29de-4593-a91e-6a02955b160e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>currentSmoker</th>\n",
       "      <th>cigsPerDay</th>\n",
       "      <th>BPMeds</th>\n",
       "      <th>prevalentStroke</th>\n",
       "      <th>prevalentHyp</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>totChol</th>\n",
       "      <th>sysBP</th>\n",
       "      <th>diaBP</th>\n",
       "      <th>BMI</th>\n",
       "      <th>heartRate</th>\n",
       "      <th>glucose</th>\n",
       "      <th>TenYearCHD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>26.97</td>\n",
       "      <td>80.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>28.73</td>\n",
       "      <td>95.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>127.5</td>\n",
       "      <td>80.0</td>\n",
       "      <td>25.34</td>\n",
       "      <td>75.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   male  age  education  currentSmoker  cigsPerDay  BPMeds  prevalentStroke  \\\n",
       "0     1   39        4.0              0         0.0     0.0                0   \n",
       "1     0   46        2.0              0         0.0     0.0                0   \n",
       "2     1   48        1.0              1        20.0     0.0                0   \n",
       "\n",
       "   prevalentHyp  diabetes  totChol  sysBP  diaBP    BMI  heartRate  glucose  \\\n",
       "0             0         0    195.0  106.0   70.0  26.97       80.0     77.0   \n",
       "1             0         0    250.0  121.0   81.0  28.73       95.0     76.0   \n",
       "2             0         0    245.0  127.5   80.0  25.34       75.0     70.0   \n",
       "\n",
       "   TenYearCHD  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8fb9772-dc32-4b92-b4bb-e4002f95a5c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4240, 16)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01bf410b-a763-4c7f-9564-b934dc61ab78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "male                 0\n",
       "age                  0\n",
       "education          105\n",
       "currentSmoker        0\n",
       "cigsPerDay          29\n",
       "BPMeds              53\n",
       "prevalentStroke      0\n",
       "prevalentHyp         0\n",
       "diabetes             0\n",
       "totChol             50\n",
       "sysBP                0\n",
       "diaBP                0\n",
       "BMI                 19\n",
       "heartRate            1\n",
       "glucose            388\n",
       "TenYearCHD           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4f13083-57b7-4412-8b6d-5de609e76d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = df.drop(['TenYearCHD'], axis=1), df.TenYearCHD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b71f2e5-82e0-47ca-bf80-32c3e4bda737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>currentSmoker</th>\n",
       "      <th>cigsPerDay</th>\n",
       "      <th>BPMeds</th>\n",
       "      <th>prevalentStroke</th>\n",
       "      <th>prevalentHyp</th>\n",
       "      <th>diabetes</th>\n",
       "      <th>totChol</th>\n",
       "      <th>sysBP</th>\n",
       "      <th>diaBP</th>\n",
       "      <th>BMI</th>\n",
       "      <th>heartRate</th>\n",
       "      <th>glucose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>26.97</td>\n",
       "      <td>80.0</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>28.73</td>\n",
       "      <td>95.0</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>127.5</td>\n",
       "      <td>80.0</td>\n",
       "      <td>25.34</td>\n",
       "      <td>75.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>28.58</td>\n",
       "      <td>65.0</td>\n",
       "      <td>103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>23.10</td>\n",
       "      <td>85.0</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4235</th>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>22.00</td>\n",
       "      <td>84.0</td>\n",
       "      <td>86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4236</th>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>210.0</td>\n",
       "      <td>126.5</td>\n",
       "      <td>87.0</td>\n",
       "      <td>19.16</td>\n",
       "      <td>86.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4237</th>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>133.5</td>\n",
       "      <td>83.0</td>\n",
       "      <td>21.47</td>\n",
       "      <td>80.0</td>\n",
       "      <td>107.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4238</th>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>25.60</td>\n",
       "      <td>67.0</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4239</th>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>196.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>20.91</td>\n",
       "      <td>85.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4240 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      male  age  education  currentSmoker  cigsPerDay  BPMeds  \\\n",
       "0        1   39        4.0              0         0.0     0.0   \n",
       "1        0   46        2.0              0         0.0     0.0   \n",
       "2        1   48        1.0              1        20.0     0.0   \n",
       "3        0   61        3.0              1        30.0     0.0   \n",
       "4        0   46        3.0              1        23.0     0.0   \n",
       "...    ...  ...        ...            ...         ...     ...   \n",
       "4235     0   48        2.0              1        20.0     NaN   \n",
       "4236     0   44        1.0              1        15.0     0.0   \n",
       "4237     0   52        2.0              0         0.0     0.0   \n",
       "4238     1   40        3.0              0         0.0     0.0   \n",
       "4239     0   39        3.0              1        30.0     0.0   \n",
       "\n",
       "      prevalentStroke  prevalentHyp  diabetes  totChol  sysBP  diaBP    BMI  \\\n",
       "0                   0             0         0    195.0  106.0   70.0  26.97   \n",
       "1                   0             0         0    250.0  121.0   81.0  28.73   \n",
       "2                   0             0         0    245.0  127.5   80.0  25.34   \n",
       "3                   0             1         0    225.0  150.0   95.0  28.58   \n",
       "4                   0             0         0    285.0  130.0   84.0  23.10   \n",
       "...               ...           ...       ...      ...    ...    ...    ...   \n",
       "4235                0             0         0    248.0  131.0   72.0  22.00   \n",
       "4236                0             0         0    210.0  126.5   87.0  19.16   \n",
       "4237                0             0         0    269.0  133.5   83.0  21.47   \n",
       "4238                0             1         0    185.0  141.0   98.0  25.60   \n",
       "4239                0             0         0    196.0  133.0   86.0  20.91   \n",
       "\n",
       "      heartRate  glucose  \n",
       "0          80.0     77.0  \n",
       "1          95.0     76.0  \n",
       "2          75.0     70.0  \n",
       "3          65.0    103.0  \n",
       "4          85.0     85.0  \n",
       "...         ...      ...  \n",
       "4235       84.0     86.0  \n",
       "4236       86.0      NaN  \n",
       "4237       80.0    107.0  \n",
       "4238       67.0     72.0  \n",
       "4239       85.0     80.0  \n",
       "\n",
       "[4240 rows x 15 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb4c81d-0880-4683-8790-98ef3ab14af7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a996aac-5126-45d3-8ea5-9843505e4b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def em_train(df, n_components=2, max_iter=10):\n",
    "    # Separate numerical and categorical columns\n",
    "    copy_df = df.copy()\n",
    "    num_cols = copy_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat_cols = copy_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    # Initialize GMM for numerical columns\n",
    "    gmm_imputer = GaussianMixture(n_components=n_components, max_iter=max_iter)\n",
    "    copy_df[num_cols] = em_impute(gmm_imputer, copy_df[num_cols], max_iter)\n",
    "    \n",
    "    # Store mode for categorical columns for test set imputation\n",
    "    cat_modes = {col: copy_df[col].mode()[0] for col in cat_cols}\n",
    "    \n",
    "    return copy_df, gmm_imputer, cat_modes\n",
    "\n",
    "def em_impute(gmm, data, max_iter):\n",
    "    data_imputed = data.copy()\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        # Fit GMM on observed data (ignoring rows with missing values in numerical columns)\n",
    "        gmm.fit(data_imputed.dropna())\n",
    "        \n",
    "        # Impute missing values using the fitted GMM\n",
    "        missing_mask = data.isna()\n",
    "        for col in data.columns:\n",
    "            col_missing = missing_mask[col]\n",
    "            if col_missing.any():  # If there are missing values in this column\n",
    "                # Generate new samples for missing values in the column\n",
    "                samples = gmm.sample(col_missing.sum())[0]\n",
    "                data_imputed.loc[col_missing, col] = samples[:, 0]  # Use only the first component for the column\n",
    "                \n",
    "    return data_imputed\n",
    "\n",
    "\n",
    "def mice_train(df, max_iter=10):\n",
    "    # Separate numerical and categorical columns\n",
    "    copy_df = df.copy()\n",
    "    num_cols = copy_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat_cols = copy_df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "    mice_imputer = IterativeImputer(max_iter=max_iter, random_state=0)\n",
    "    copy_df[num_cols] = mice_imputer.fit_transform(copy_df[num_cols])\n",
    "    return copy_df, mice_imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ed930dee-3a0a-4219-9a35-8983d9534cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clustered_imputation import clusterImputer\n",
    "clustered_mice_X,clustered_em_X = X.copy(),X.copy()\n",
    "mice_X, mice_imputer = mice_train(X)\n",
    "em_X, em_imputer,_ = em_train(X, n_components=2, max_iter=10)\n",
    "clustered_mice = clusterImputer(clustered_mice_X , \"mice\" ,\"mean\" , 0.4 ,10)\n",
    "clustered_em = clusterImputer(clustered_em_X , \"em\" ,\"mean\" , 0.4 ,10)\n",
    "clustered_mice.impute()\n",
    "clustered_em.impute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3163e972-859f-42c9-a151-1c804b2033e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "male               0\n",
       "age                0\n",
       "education          0\n",
       "currentSmoker      0\n",
       "cigsPerDay         0\n",
       "BPMeds             0\n",
       "prevalentStroke    0\n",
       "prevalentHyp       0\n",
       "diabetes           0\n",
       "totChol            0\n",
       "sysBP              0\n",
       "diaBP              0\n",
       "BMI                0\n",
       "heartRate          0\n",
       "glucose            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustered_em_X.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e50dd386-7fee-4b53-8595-8dc9dd6b5528",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "male               0\n",
       "age                0\n",
       "education          0\n",
       "currentSmoker      0\n",
       "cigsPerDay         0\n",
       "BPMeds             0\n",
       "prevalentStroke    0\n",
       "prevalentHyp       0\n",
       "diabetes           0\n",
       "totChol            0\n",
       "sysBP              0\n",
       "diaBP              0\n",
       "BMI                0\n",
       "heartRate          0\n",
       "glucose            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustered_mice_X.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "21ef1e53-3127-4ba2-80dc-ac0d0e942e3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "male               0\n",
       "age                0\n",
       "education          0\n",
       "currentSmoker      0\n",
       "cigsPerDay         0\n",
       "BPMeds             0\n",
       "prevalentStroke    0\n",
       "prevalentHyp       0\n",
       "diabetes           0\n",
       "totChol            0\n",
       "sysBP              0\n",
       "diaBP              0\n",
       "BMI                0\n",
       "heartRate          0\n",
       "glucose            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em_X.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6308b786-4550-4504-b3b5-0214fee1da8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "male               0\n",
       "age                0\n",
       "education          0\n",
       "currentSmoker      0\n",
       "cigsPerDay         0\n",
       "BPMeds             0\n",
       "prevalentStroke    0\n",
       "prevalentHyp       0\n",
       "diabetes           0\n",
       "totChol            0\n",
       "sysBP              0\n",
       "diaBP              0\n",
       "BMI                0\n",
       "heartRate          0\n",
       "glucose            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mice_X.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b172ba1d-1f98-453c-9772-a253085421a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_kfold_model(X, y, X_test, ModelClass, params):\n",
    "    kf = KFold(n_splits=5, random_state=0, shuffle=True)\n",
    "    models = []\n",
    "    X_test_preds = np.zeros(X_test.shape[0])  # Initialize with zeros\n",
    "    X_train_loss = 0\n",
    "    X_val_loss = 0\n",
    "    y = np.array(y)\n",
    "    for train_index, val_index in kf.split(X):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]  # Access y as a NumPy array\n",
    "\n",
    "        model = ModelClass(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        train_preds = model.predict(X_train)\n",
    "        val_preds = model.predict(X_val)\n",
    "        test_preds = model.predict(X_test)\n",
    "\n",
    "        train_loss = accuracy_score(y_train, train_preds)\n",
    "        val_loss = accuracy_score(y_val, val_preds)\n",
    "        X_train_loss += train_loss\n",
    "        X_val_loss += val_loss\n",
    "        X_test_preds += test_preds\n",
    "\n",
    "        models.append(model)\n",
    "\n",
    "    # Average predictions for test set\n",
    "    X_test_preds /= kf.n_splits\n",
    "    X_test_preds = np.where(X_test_preds < 0.5, 0, 1)  # Convert to binary predictions\n",
    "\n",
    "    return models, X_test_preds, X_train_loss / kf.n_splits, X_val_loss / kf.n_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d0f23251-47cf-4c39-bf38-2e9b83ff4417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_kfold_results_am():\n",
    "    models = {\n",
    "        'XGB': {\n",
    "        'model': XGBClassifier,\n",
    "        'params':  {'n_estimators': 1000, 'max_depth': 8, 'alpha': 1.9738594118973054, 'lambda': 4.009774721904594, 'learning_rate': 0.06723886268535728, 'subsample': 0.8046187868546699, 'colsample_bytree': 0.6422765532436576},\n",
    "        'trained_models': [],\n",
    "        'test_preds' : []\n",
    "    },\n",
    "       'LGBM': {\n",
    "        'model': LGBMClassifier,\n",
    "        'params': {'lambda': 0.012092546196007962, 'alpha': 31.687502527930103, 'num_leaves': 91, 'n_estimators': 1000, 'max_depth': 6, 'learning_rate': 0.3393288299428538, 'bagging_fraction': 0.9195007323386031, 'mi_split_gain': 0.03928856728583821, 'lambda_l1': 0.10440536035552132, 'lambda_l2': 0.19185454581480765,'verbose':-100},\n",
    "        'trained_models': [],   \n",
    "        'test_preds' : []\n",
    "    },    \n",
    "    'HIST' :{\n",
    "        'model' : HistGradientBoostingClassifier,\n",
    "        'params':{'learning_rate': 2.4832513288186244, 'max_leaf_nodes': 31, 'l2_regularization': 8.930490898699928, 'max_depth': 10, 'min_samples_leaf': 50},\n",
    "        'trained_models': [],\n",
    "        'test_preds' : []\n",
    "    },\n",
    "        'RF': {\n",
    "        'model' : RandomForestClassifier,\n",
    "        'params': {'n_estimators': 1000, 'max_depth': 9, 'class_weight': {0: 1, 1: 5}, 'min_samples_split': 6},\n",
    "        'trained_models': [],\n",
    "        'test_preds' : []\n",
    "        }\n",
    "  }\n",
    "    X = x_train_clustered_mice\n",
    "    y = y_train_clustered_mice\n",
    "    new_test_df = x_test_clustered_mice\n",
    "    for i in models:\n",
    "        trained_models,X_test_preds,X_train_score,X_val_score = build_kfold_model(X,y,new_test_df,models[i]['model'],models[i]['params'])\n",
    "        models[i]['trained_models'] = trained_models\n",
    "        models[i]['test_preds'] = X_test_preds\n",
    "        print(f\"Accuracy Score For mode {i} is --- Train accuracy {X_train_score} and --- Val accuracy {X_val_score}\")\n",
    "\n",
    "    return models\n",
    "\n",
    "\n",
    "def get_model_kfold_results_em():\n",
    "    models = {\n",
    "        'XGB': {\n",
    "        'model': XGBClassifier,\n",
    "        'params':   {'n_estimators': 1000, 'max_depth': 8, 'alpha': 1.9738594118973054, 'lambda': 4.009774721904594, 'learning_rate': 0.06723886268535728, 'subsample': 0.8046187868546699, 'colsample_bytree': 0.6422765532436576},\n",
    "        'trained_models': [],\n",
    "        'test_preds' : []\n",
    "    },\n",
    "       'LGBM': {\n",
    "        'model': LGBMClassifier,\n",
    "        'params':  {'lambda': 0.012092546196007962, 'alpha': 31.687502527930103, 'num_leaves': 91, 'n_estimators': 1000, 'max_depth': 6, 'learning_rate': 0.3393288299428538, 'bagging_fraction': 0.9195007323386031, 'mi_split_gain': 0.03928856728583821, 'lambda_l1': 0.10440536035552132, 'lambda_l2': 0.19185454581480765,'verbose':-100},\n",
    "        'trained_models': [],   \n",
    "        'test_preds' : []\n",
    "    },    \n",
    "    'HIST' :{\n",
    "        'model' : HistGradientBoostingClassifier,\n",
    "        'params':  {'learning_rate': 2.4832513288186244, 'max_leaf_nodes': 31, 'l2_regularization': 8.930490898699928, 'max_depth': 10, 'min_samples_leaf': 50},\n",
    "        'trained_models': [],\n",
    "        'test_preds' : []\n",
    "    },\n",
    "        'RF': {\n",
    "        'model' : RandomForestClassifier,\n",
    "        'params':{'n_estimators': 1000, 'max_depth': 9, 'class_weight': {0: 1, 1: 5}, 'min_samples_split': 6},\n",
    "        'trained_models': [],\n",
    "        'test_preds' : []\n",
    "        }\n",
    "  }\n",
    "    X = x_train_clustered_em\n",
    "    y = y_train_clustered_em\n",
    "    new_test_df = np.array(x_test_clustered_em)\n",
    "    for i in models:\n",
    "        trained_models,X_test_preds,X_train_score,X_val_score = build_kfold_model(X,y,new_test_df,models[i]['model'],models[i]['params'])\n",
    "        models[i]['trained_models'] = trained_models\n",
    "        models[i]['test_preds'] = X_test_preds\n",
    "        print(f\"Accuracy Score For mode {i} is --- Train accuracy {X_train_score} and --- Val accuracy {X_val_score}\")\n",
    "\n",
    "    return models\n",
    "\n",
    "\n",
    "\n",
    "def get_model_kfold_results_plain_mice():\n",
    "    models = {\n",
    "        'XGB': {\n",
    "        'model': XGBClassifier,\n",
    "        'params':  {'n_estimators': 1000, 'max_depth': 8, 'alpha': 1.9738594118973054, 'lambda': 4.009774721904594, 'learning_rate': 0.06723886268535728, 'subsample': 0.8046187868546699, 'colsample_bytree': 0.6422765532436576},\n",
    "        'trained_models': [],\n",
    "        'test_preds' : []\n",
    "    },\n",
    "       'LGBM': {\n",
    "        'model': LGBMClassifier,\n",
    "        'params':{'lambda': 0.012092546196007962, 'alpha': 31.687502527930103, 'num_leaves': 91, 'n_estimators': 1000, 'max_depth': 6, 'learning_rate': 0.3393288299428538, 'bagging_fraction': 0.9195007323386031, 'mi_split_gain': 0.03928856728583821, 'lambda_l1': 0.10440536035552132, 'lambda_l2': 0.19185454581480765,'verbose':-100},\n",
    "        'trained_models': [],   \n",
    "        'test_preds' : []\n",
    "    },    \n",
    "    'HIST' :{\n",
    "        'model' : HistGradientBoostingClassifier,\n",
    "        'params': {'learning_rate': 2.4832513288186244, 'max_leaf_nodes': 31, 'l2_regularization': 8.930490898699928, 'max_depth': 10, 'min_samples_leaf': 50},\n",
    "        'trained_models': [],\n",
    "        'test_preds' : []\n",
    "    },\n",
    "        'RF': {\n",
    "        'model' : RandomForestClassifier,\n",
    "        'params': {'n_estimators': 1000, 'max_depth': 9, 'class_weight': {0: 1, 1: 5}, 'min_samples_split': 6},\n",
    "        'trained_models': [],\n",
    "        'test_preds' : []\n",
    "        }\n",
    "  }\n",
    "    X = x_train_plain_mice\n",
    "    y = y_train_plain_mice\n",
    "    new_test_df = np.array(x_test_plain_mice)\n",
    "    for i in models:\n",
    "        trained_models,X_test_preds,X_train_score,X_val_score = build_kfold_model(X,y,new_test_df,models[i]['model'],models[i]['params'])\n",
    "        models[i]['trained_models'] = trained_models\n",
    "        models[i]['test_preds'] = X_test_preds\n",
    "        print(f\"Accuracy Score For mode {i} is --- Train accuracy {X_train_score} and --- Val accuracy {X_val_score}\")\n",
    "\n",
    "    return models\n",
    "\n",
    "\n",
    "\n",
    "def get_model_kfold_results_plain_em():\n",
    "    models = {\n",
    "        'XGB': {\n",
    "        'model': XGBClassifier,\n",
    "        'params':   {'n_estimators': 1000, 'max_depth': 8, 'alpha': 1.9738594118973054, 'lambda': 4.009774721904594, 'learning_rate': 0.06723886268535728, 'subsample': 0.8046187868546699, 'colsample_bytree': 0.6422765532436576},\n",
    "        'trained_models': [],\n",
    "        'test_preds' : []\n",
    "    },\n",
    "       'LGBM': {\n",
    "        'model': LGBMClassifier,\n",
    "        'params': {'lambda': 0.012092546196007962, 'alpha': 31.687502527930103, 'num_leaves': 91, 'n_estimators': 1000, 'max_depth': 6, 'learning_rate': 0.3393288299428538, 'bagging_fraction': 0.9195007323386031, 'mi_split_gain': 0.03928856728583821, 'lambda_l1': 0.10440536035552132, 'lambda_l2': 0.19185454581480765,'verbose':-100},\n",
    "        'trained_models': [],   \n",
    "        'test_preds' : []\n",
    "    },    \n",
    "    'HIST' :{\n",
    "        'model' : HistGradientBoostingClassifier,\n",
    "        'params':  {'learning_rate': 2.4832513288186244, 'max_leaf_nodes': 31, 'l2_regularization': 8.930490898699928, 'max_depth': 10, 'min_samples_leaf': 50},\n",
    "        'trained_models': [],\n",
    "        'test_preds' : []\n",
    "    },\n",
    "        'RF': {\n",
    "        'model' : RandomForestClassifier,\n",
    "        'params': {'n_estimators': 1000, 'max_depth': 9, 'class_weight': {0: 1, 1: 5}, 'min_samples_split': 6},\n",
    "        'trained_models': [],\n",
    "        'test_preds' : []\n",
    "        }\n",
    "  }\n",
    "    X = x_train_plain_em\n",
    "    y = y_train_plain_em\n",
    "    new_test_df = np.array(x_test_plain_em)\n",
    "    for i in models:\n",
    "        trained_models,X_test_preds,X_train_score,X_val_score = build_kfold_model(X,y,new_test_df,models[i]['model'],models[i]['params'])\n",
    "        models[i]['trained_models'] = trained_models\n",
    "        models[i]['test_preds'] = X_test_preds\n",
    "        print(f\"Accuracy Score For mode {i} is --- Train accuracy {X_train_score} and --- Val accuracy {X_val_score}\")\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c21c6198-a626-432f-81c5-43ee68252727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_am(X_train, y_train, X_test, y_test):\n",
    "    models = {\n",
    "        'XGB': {\n",
    "        'model': XGBClassifier,\n",
    "        'params':   {'n_estimators': 1000, 'max_depth': 8, 'alpha': 1.9738594118973054, 'lambda': 4.009774721904594, 'learning_rate': 0.06723886268535728, 'subsample': 0.8046187868546699, 'colsample_bytree': 0.6422765532436576},\n",
    "        'trained_models': [],\n",
    "        'test_preds' : []\n",
    "    },\n",
    "       'LGBM': {\n",
    "        'model': LGBMClassifier,\n",
    "        'params': {'lambda': 0.012092546196007962, 'alpha': 31.687502527930103, 'num_leaves': 91, 'n_estimators': 1000, 'max_depth': 6, 'learning_rate': 0.3393288299428538, 'bagging_fraction': 0.9195007323386031, 'mi_split_gain': 0.03928856728583821, 'lambda_l1': 0.10440536035552132, 'lambda_l2': 0.19185454581480765,'verbose':-100},\n",
    "        'trained_models': [],   \n",
    "        'test_preds' : []\n",
    "    },    \n",
    "    'HIST' :{\n",
    "        'model' : HistGradientBoostingClassifier,\n",
    "        'params':  {'learning_rate': 2.4832513288186244, 'max_leaf_nodes': 31, 'l2_regularization': 8.930490898699928, 'max_depth': 10, 'min_samples_leaf': 50},\n",
    "        'trained_models': [],\n",
    "        'test_preds' : []\n",
    "    },\n",
    "        'RF': {\n",
    "        'model' : RandomForestClassifier,\n",
    "        'params': {'n_estimators': 1000, 'max_depth': 9, 'class_weight': {0: 1, 1: 5}, 'min_samples_split': 6},\n",
    "        'trained_models': [],\n",
    "        'test_preds' : []\n",
    "        }\n",
    "  }\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "    \n",
    "    for model_name, model_dict in models.items():\n",
    "        model = model_dict['model'](**model_dict['params'])  # Instantiate model\n",
    "        model.fit(X_train, y_train)  # Train the model\n",
    "        \n",
    "        # Make predictions\n",
    "        preds = model.predict(X_test)\n",
    "        \n",
    "        # Calculate accuracy score\n",
    "        accuracy = accuracy_score(y_test, preds)\n",
    "        print(f\"{model_name} Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "def predict_em(X_train, y_train, X_test, y_test):\n",
    "    models = {\n",
    "        'XGB': {\n",
    "        'model': XGBClassifier,\n",
    "        'params':   {'n_estimators': 1000, 'max_depth': 8, 'alpha': 1.9738594118973054, 'lambda': 4.009774721904594, 'learning_rate': 0.06723886268535728, 'subsample': 0.8046187868546699, 'colsample_bytree': 0.6422765532436576},\n",
    "        'trained_models': [],\n",
    "        'test_preds' : []\n",
    "    },\n",
    "       'LGBM': {\n",
    "        'model': LGBMClassifier,\n",
    "        'params': {'lambda': 0.012092546196007962, 'alpha': 31.687502527930103, 'num_leaves': 91, 'n_estimators': 1000, 'max_depth': 6, 'learning_rate': 0.3393288299428538, 'bagging_fraction': 0.9195007323386031, 'mi_split_gain': 0.03928856728583821, 'lambda_l1': 0.10440536035552132, 'lambda_l2': 0.19185454581480765,'verbose':-100},\n",
    "        'trained_models': [],   \n",
    "        'test_preds' : []\n",
    "    },    \n",
    "    'HIST' :{\n",
    "        'model' : HistGradientBoostingClassifier,\n",
    "        'params':  {'learning_rate': 2.4832513288186244, 'max_leaf_nodes': 31, 'l2_regularization': 8.930490898699928, 'max_depth': 10, 'min_samples_leaf': 50},\n",
    "        'trained_models': [],\n",
    "        'test_preds' : []\n",
    "    },\n",
    "        'RF': {\n",
    "        'model' : RandomForestClassifier,\n",
    "        'params': {'n_estimators': 1000, 'max_depth': 9, 'class_weight': {0: 1, 1: 5}, 'min_samples_split': 6},\n",
    "        'trained_models': [],\n",
    "        'test_preds' : []\n",
    "        }\n",
    "  }\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    for model_name, model_dict in models.items():\n",
    "        model = model_dict['model'](**model_dict['params'])  # Instantiate model\n",
    "        model.fit(X_train, y_train)  # Train the model\n",
    "        \n",
    "        # Make predictions\n",
    "        preds = model.predict(X_test)\n",
    "        \n",
    "        # Calculate accuracy score\n",
    "        accuracy = accuracy_score(y_test, preds)\n",
    "        print(f\"{model_name} Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "def predict_plain_mice(X_train, y_train, X_test, y_test):\n",
    "    models = {\n",
    "        'XGB': {\n",
    "        'model': XGBClassifier,\n",
    "        'params':   {'n_estimators': 1000, 'max_depth': 8, 'alpha': 1.9738594118973054, 'lambda': 4.009774721904594, 'learning_rate': 0.06723886268535728, 'subsample': 0.8046187868546699, 'colsample_bytree': 0.6422765532436576},\n",
    "        'trained_models': [],\n",
    "        'test_preds' : []\n",
    "    },\n",
    "       'LGBM': {\n",
    "        'model': LGBMClassifier,\n",
    "        'params': {'lambda': 0.012092546196007962, 'alpha': 31.687502527930103, 'num_leaves': 91, 'n_estimators': 1000, 'max_depth': 6, 'learning_rate': 0.3393288299428538, 'bagging_fraction': 0.9195007323386031, 'mi_split_gain': 0.03928856728583821, 'lambda_l1': 0.10440536035552132, 'lambda_l2': 0.19185454581480765,'verbose':-100},\n",
    "        'trained_models': [],   \n",
    "        'test_preds' : []\n",
    "    },    \n",
    "    'HIST' :{\n",
    "        'model' : HistGradientBoostingClassifier,\n",
    "        'params':  {'learning_rate': 2.4832513288186244, 'max_leaf_nodes': 31, 'l2_regularization': 8.930490898699928, 'max_depth': 10, 'min_samples_leaf': 50},\n",
    "        'trained_models': [],\n",
    "        'test_preds' : []\n",
    "    },\n",
    "        'RF': {\n",
    "        'model' : RandomForestClassifier,\n",
    "        'params': {'n_estimators': 1000, 'max_depth': 9, 'class_weight': {0: 1, 1: 5}, 'min_samples_split': 6},\n",
    "        'trained_models': [],\n",
    "        'test_preds' : []\n",
    "        }\n",
    "  }\n",
    "\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    for model_name, model_dict in models.items():\n",
    "        model = model_dict['model'](**model_dict['params'])  # Instantiate model\n",
    "        model.fit(X_train, y_train)  # Train the model\n",
    "        \n",
    "        # Make predictions\n",
    "        preds = model.predict(X_test)\n",
    "        \n",
    "        # Calculate accuracy score\n",
    "        accuracy = accuracy_score(y_test, preds)\n",
    "        print(f\"{model_name} Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "\n",
    "def predict_plain_em(X_train, y_train, X_test, y_test):\n",
    "    models = {\n",
    "        'XGB': {\n",
    "        'model': XGBClassifier,\n",
    "        'params':   {'n_estimators': 1000, 'max_depth': 8, 'alpha': 1.9738594118973054, 'lambda': 4.009774721904594, 'learning_rate': 0.06723886268535728, 'subsample': 0.8046187868546699, 'colsample_bytree': 0.6422765532436576},\n",
    "        'trained_models': [],\n",
    "        'test_preds' : []\n",
    "    },\n",
    "       'LGBM': {\n",
    "        'model': LGBMClassifier,\n",
    "        'params': {'lambda': 0.012092546196007962, 'alpha': 31.687502527930103, 'num_leaves': 91, 'n_estimators': 1000, 'max_depth': 6, 'learning_rate': 0.3393288299428538, 'bagging_fraction': 0.9195007323386031, 'mi_split_gain': 0.03928856728583821, 'lambda_l1': 0.10440536035552132, 'lambda_l2': 0.19185454581480765,'verbose':-100},\n",
    "        'trained_models': [],   \n",
    "        'test_preds' : []\n",
    "    },    \n",
    "    'HIST' :{\n",
    "        'model' : HistGradientBoostingClassifier,\n",
    "        'params':  {'learning_rate': 2.4832513288186244, 'max_leaf_nodes': 31, 'l2_regularization': 8.930490898699928, 'max_depth': 10, 'min_samples_leaf': 50},\n",
    "        'trained_models': [],\n",
    "        'test_preds' : []\n",
    "    },\n",
    "        'RF': {\n",
    "        'model' : RandomForestClassifier,\n",
    "        'params': {'n_estimators': 1000, 'max_depth': 9, 'class_weight': {0: 1, 1: 5}, 'min_samples_split': 6},\n",
    "        'trained_models': [],\n",
    "        'test_preds' : []\n",
    "        }\n",
    "  }\n",
    "    X_test = np.array(X_test)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    for model_name, model_dict in models.items():\n",
    "        model = model_dict['model'](**model_dict['params'])  # Instantiate model\n",
    "        model.fit(X_train, y_train)  # Train the model\n",
    "        \n",
    "        # Make predictions\n",
    "        preds = model.predict(X_test)\n",
    "        \n",
    "        # Calculate accuracy score\n",
    "        accuracy = accuracy_score(y_test, preds)\n",
    "        print(f\"{model_name} Model Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d8ec3a67-383f-4101-9423-8c4efe4d4e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_plain_mice,x_test_plain_mice,y_train_plain_mice,y_test_plain_mice = train_test_split(mice_X,y,random_state=42,stratify=y,test_size = 0.3)\n",
    "x_train_plain_em,x_test_plain_em,y_train_plain_em,y_test_plain_em = train_test_split(em_X,y,random_state=42,stratify=y,test_size = 0.3)\n",
    "x_train_clustered_mice,x_test_clustered_mice,y_train_clustered_mice,y_test_clustered_mice = train_test_split(clustered_mice_X,y,random_state=42,stratify=y,test_size = 0.3)\n",
    "x_train_clustered_em,x_test_clustered_em,y_train_clustered_em,y_test_clustered_em = train_test_split(clustered_em_X,y,random_state=42,stratify=y,test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cb6af9c1-3194-408f-b0e7-8d7426b31742",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_corr_mice = StandardScaler()\n",
    "scaler_corr_em = StandardScaler()\n",
    "scaler_plain_mice = StandardScaler()\n",
    "scaler_plain_em = StandardScaler()\n",
    "\n",
    "# Apply StandardScaler on correlation-based MICE imputed data\n",
    "x_train_clustered_mice = scaler_corr_mice.fit_transform(x_train_clustered_mice)\n",
    "x_test_clustered_mice = scaler_corr_mice.transform(x_test_clustered_mice)\n",
    "\n",
    "# Apply StandardScaler on correlation-based EM imputed data\n",
    "x_train_clustered_em = scaler_corr_em.fit_transform(x_train_clustered_em)\n",
    "x_test_clustered_em = scaler_corr_em.transform(x_test_clustered_em)\n",
    "\n",
    "# Apply StandardScaler on plain MICE imputed data\n",
    "x_train_plain_mice = scaler_plain_mice.fit_transform(x_train_plain_mice)\n",
    "x_test_plain_mice = scaler_plain_mice.transform(x_test_plain_mice)\n",
    "\n",
    "# Apply StandardScaler on plain EM imputed data\n",
    "x_train_plain_em = scaler_plain_em.fit_transform(x_train_plain_em)\n",
    "x_test_plain_em = scaler_plain_em.transform(x_test_plain_em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0d831971-bd7b-4b5a-a0ae-f13c184ce143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score For mode XGB is --- Train accuracy 0.9983995743360085 and --- Val accuracy 0.8355851942698486\n",
      "Accuracy Score For mode LGBM is --- Train accuracy 1.0 and --- Val accuracy 0.8298578817971733\n",
      "Accuracy Score For mode HIST is --- Train accuracy 0.9882055602358889 and --- Val accuracy 0.7900948779532253\n",
      "Accuracy Score For mode RF is --- Train accuracy 0.9438177803396444 and --- Val accuracy 0.8140187711857187\n"
     ]
    }
   ],
   "source": [
    "results_am = get_model_kfold_results_am()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3894224f-9649-477c-87a7-52c9f75735c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Model Accuracy: 0.8325\n",
      "LGBM Model Accuracy: 0.8192\n",
      "HIST Model Accuracy: 0.7987\n",
      "RF Model Accuracy: 0.7964\n"
     ]
    }
   ],
   "source": [
    "predict_am(x_train_clustered_mice,y_train_clustered_mice  ,x_test_clustered_mice , y_test_clustered_mice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e2a20a3c-4621-4052-8e75-bca9975d2afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score For mode XGB is --- Train accuracy 0.9983995743360085 and --- Val accuracy 0.8355851942698486\n",
      "Accuracy Score For mode LGBM is --- Train accuracy 1.0 and --- Val accuracy 0.8298578817971733\n",
      "Accuracy Score For mode HIST is --- Train accuracy 0.9882055602358889 and --- Val accuracy 0.7900948779532253\n",
      "Accuracy Score For mode RF is --- Train accuracy 0.944912552653749 and --- Val accuracy 0.8146927396505811\n"
     ]
    }
   ],
   "source": [
    "results_em = get_model_kfold_results_em()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7870372f-a7cc-4dbc-80ec-8bf6e2401a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Model Accuracy: 0.8310\n",
      "LGBM Model Accuracy: 0.8263\n",
      "HIST Model Accuracy: 0.7728\n",
      "RF Model Accuracy: 0.7925\n"
     ]
    }
   ],
   "source": [
    "predict_plain_mice(x_train_plain_mice,y_train_plain_mice  ,x_test_plain_mice , y_test_plain_mice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fc455ad7-0bf0-4aaa-9089-df3df89a4995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score For mode XGB is --- Train accuracy 0.9990734713785306 and --- Val accuracy 0.838953333219775\n",
      "Accuracy Score For mode LGBM is --- Train accuracy 1.0 and --- Val accuracy 0.8349106580135247\n",
      "Accuracy Score For mode HIST is --- Train accuracy 0.9574706690905866 and --- Val accuracy 0.7877260519756304\n",
      "Accuracy Score For mode RF is --- Train accuracy 0.9377524941249501 and --- Val accuracy 0.8099795027282379\n"
     ]
    }
   ],
   "source": [
    "results_em_plain = get_model_kfold_results_plain_em()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "92abdd66-fab1-44b0-b434-35a989b66963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Model Accuracy: 0.8294\n",
      "LGBM Model Accuracy: 0.8255\n",
      "HIST Model Accuracy: 0.7909\n",
      "RF Model Accuracy: 0.7940\n"
     ]
    }
   ],
   "source": [
    "predict_plain_em(x_train_plain_em,y_train_plain_em  ,x_test_plain_em , y_test_plain_em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164a2402-fd60-4604-9848-2b7df55394d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
